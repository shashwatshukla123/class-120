{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "class-120.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvXF7oIPjc1SzLEAKaxKi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashwatshukla123/class-120/blob/main/class_120.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA7GkkVeZLXa"
      },
      "source": [
        "## Na誰ve: \n",
        "It is called Na誰ve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features.\n",
        "\\\n",
        " Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
        "\\\n",
        "It is a superised machine learnig algorithm based on bayes probability threom.\n",
        "\\\n",
        "Na誰ve Bayes assume that there is no corelation betwwen the feature in a dataset used to train the model.\n",
        "\\\n",
        "Bayes threom describe the probility of an event based n prior knewledge or coditions that might be related to the events.\n",
        "\\\n",
        "example: If the probilty of someone having diabetes is related to his her age , then by useing the bayes threom the age can be used to predict probability of having diabetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpyPWk8qfAMY"
      },
      "source": [
        "import random\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import statistics\n",
        "import csv\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO \n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzwh-9CzfDSO",
        "outputId": "70ab12a4-5794-4a73-fed2-53c0f9d0f660"
      },
      "source": [
        "df = pd.read_csv('diabetes2.csv')\n",
        "print(df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   glucose  bloodpressure  diabetes\n",
            "0       40             85         0\n",
            "1       40             92         0\n",
            "2       45             63         1\n",
            "3       45             80         0\n",
            "4       40             73         1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjyKvBOta1ws"
      },
      "source": [
        "x = df[['bloodpressure','glucose']]\n",
        "y = df['diabetes']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ycr8dTBgtBx"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSgpR7eYio9-",
        "outputId": "edf91293-15c8-4a2c-f982-8855426c8187"
      },
      "source": [
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.fit_transform(x_test) \n",
        "model = GaussianNB()\n",
        "model.fit(x_train, y_train) \n",
        "y_pred = model.predict(x_test) \n",
        "accuracy = accuracy_score(y_test, y_pred) \n",
        "print(accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9437751004016064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7QJMYwvivZj",
        "outputId": "8ddf89c2-2cc9-4e31-c31f-4bbd7e97147d"
      },
      "source": [
        "# Apply logistic regression\n",
        "x = df[['bloodpressure','glucose']]\n",
        "y = df['diabetes']\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x, y, test_size=0.25, random_state=42)\n",
        "sc = StandardScaler()\n",
        "x_train_2 = sc.fit_transform(x_train_2)\n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9156626506024096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_XXcTtEj8xa"
      },
      "source": [
        "# Conclusion\n",
        "While the accuracy score for both the datasets was close, with Naive Bayes giving us an accuracy of 94.4% and logistic regression giving us an accuracy of 91.6%, Naive Bayes still performed better. The reason for this is that if we look at our features again, we can see that the Glucose and the Blood Pressure had no correlation with each other. They both contributed individually to whether a person would have diabetes or not. This is exactly what Naive Bayes algorithm assumes, that all the features contribute individually to the outcome. This was for the case of where Naive Bayes outperforms Logistic Regression, but let's see an example of the case where Logistic Regression outperforms Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebY0dFOikq4J",
        "outputId": "72b9e417-59b0-44bb-9c93-f5ce7998d70a"
      },
      "source": [
        "df = pd.read_csv('income.csv')\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          workclass  ...  native-country  income\n",
            "0   39          State-gov  ...   United-States   <=50K\n",
            "1   50   Self-emp-not-inc  ...   United-States   <=50K\n",
            "2   38            Private  ...   United-States   <=50K\n",
            "3   53            Private  ...   United-States   <=50K\n",
            "4   28            Private  ...            Cuba   <=50K\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "                age  education-num  capital-gain  capital-loss  hours-per-week\n",
            "count  45222.000000   45222.000000  45222.000000  45222.000000    45222.000000\n",
            "mean      38.547941      10.118460   1101.430344     88.595418       40.938017\n",
            "std       13.217870       2.552881   7506.430084    404.956092       12.007508\n",
            "min       17.000000       1.000000      0.000000      0.000000        1.000000\n",
            "25%       28.000000       9.000000      0.000000      0.000000       40.000000\n",
            "50%       37.000000      10.000000      0.000000      0.000000       40.000000\n",
            "75%       47.000000      13.000000      0.000000      0.000000       45.000000\n",
            "max       90.000000      16.000000  99999.000000   4356.000000       99.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_omxkOplGRz"
      },
      "source": [
        "From the given data, we will consider the following fields to determine the salary of a person - Age \n",
        "\\\n",
        "Hours Per Week\n",
        "\\\n",
        " Education Number \n",
        " \\\n",
        " Capital Gain \n",
        " \\\n",
        " Capital Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN6_E2aklUKx"
      },
      "source": [
        "X = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]] \n",
        "y = df[\"income\"]\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHR57lZUlhOD",
        "outputId": "939cfa0a-745a-4450-b310-4c0a4723ba6b"
      },
      "source": [
        "sc = StandardScaler()\n",
        "x_train_1 = sc.fit_transform(x_train_1)\n",
        "x_test_1 = sc.fit_transform(x_test_1) \n",
        "model = GaussianNB()\n",
        "model.fit(x_train_1, y_train_1) \n",
        "y_pred_1 = model.predict(x_test_1) \n",
        "accuracy = accuracy_score(y_test_1, y_pred_1) \n",
        "print(accuracy)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7896692021935255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5FU3lTSl-iF",
        "outputId": "5311b3ee-ce31-447d-e6a0-77f7870a5ce8"
      },
      "source": [
        "# Apply logistic regression\n",
        "x = df[[\"age\", \"hours-per-week\", \"education-num\", \"capital-gain\", \"capital-loss\"]]\n",
        "y = df['income']\n",
        "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x, y, test_size=0.25, random_state=42)\n",
        "sc = StandardScaler()\n",
        "x_train_2 = sc.fit_transform(x_train_2)\n",
        "x_test_2 = sc.fit_transform(x_test_2) \n",
        "model_2 = LogisticRegression(random_state = 0) \n",
        "model_2.fit(x_train_2, y_train_2)\n",
        "y_pred_2 = model_2.predict(x_test_2)\n",
        "accuracy = accuracy_score(y_test_2, y_pred_2)\n",
        "print(accuracy)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8116929064213692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InfWCODKl7_0"
      },
      "source": [
        "While the accuracy score for both the datasets was close, with Naive Bayes giving us an accuracy of 78.9% and logistic regression giving us an accuracy of 81.1%, Logistic regression still performed better. \n",
        "\\\n",
        "The reason is that in this dataset, not all features contribute individually to the outcome. For example, there have been people of all age groups earning both less than and more than 50K. There have also been people with all education numbers that have an income of both less and more than 50K. Here, the combination of all the features is a better predictor of whether a person is earning more than or less than 50K, instead of all features having their individual contribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr7SY7qenP-m"
      },
      "source": [
        "# Conclusion\n",
        "IF they are co-related use logistic regression, if not use Na誰ve Bayes for better predicuion."
      ]
    }
  ]
}